import { exec } from "child_process";
import cors from "cors";
import dotenv from "dotenv";
import express from "express";
import { promises as fs } from "fs";
import OpenAI from "openai";
import path from "path";
import axios from "axios"; // Need to install: npm install axios
import os from "os";

dotenv.config();

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY || "-",
});

const app = express();
app.use(express.json());
app.use(cors());  
const port = 3000;

// Python API endpoint for database operations
const PYTHON_API_URL = process.env.PYTHON_API_URL || "http://localhost:8000";

app.get("/", (req, res) => {
  res.send("AI Work Assistant Server is running!");
});

const execCommand = (command) => {
  return new Promise((resolve, reject) => {
    exec(command, (error, stdout, stderr) => {
      if (error) reject(error);
      resolve(stdout);
    });
  });
};

const lipSyncMessage = async (fileBaseName) => {
  const time = new Date().getTime();
  console.log(`Starting conversion for ${fileBaseName}`);
  
  const mp3Path = `audios/${fileBaseName}.mp3`;
  const wavPath = `audios/${fileBaseName}.wav`;
  const jsonPath = `audios/${fileBaseName}.json`;
  
  await execCommand(
    `ffmpeg -y -i ${mp3Path} ${wavPath}`
    // -y to overwrite the file
  );
  console.log(`Conversion done in ${new Date().getTime() - time}ms`);

  const platform = os.platform();
  console.log(`Platform detected: ${platform}`);

  if (platform === 'win32') {
    // For Windows
    const currentDir = process.cwd();
    const rhubarbPath = path.join(currentDir, 'rhubarb', 'rhubarb.exe');
    await execCommand(
      `"${rhubarbPath}" -f json -o ${jsonPath} ${wavPath} -r phonetic`
    );
  } else if (platform === 'darwin') {
    // For MacOS with arm64 - need to install softwareupdate --install-rosetta
    await execCommand(
      `arch -x86_64 ./bin/rhubarb -f json -o ${jsonPath} ${wavPath} -r phonetic`
    );
  } else {
    // For Linux or other platforms, try the Linux version
    await execCommand(
      `./bin/rhubarb -f json -o ${jsonPath} ${wavPath} -r phonetic`
    );
  }

  // -r phonetic is faster but less accurate
  console.log(`Lip sync done in ${new Date().getTime() - time}ms`);
};

// Function to generate speech using OpenAI's text-to-speech API
const generateSpeech = async (text, fileName, voice = "nova") => {
  try {
    // Available voices: alloy, echo, fable, onyx, nova, shimmer
    const mp3 = await openai.audio.speech.create({
      model: "tts-1",
      voice: voice,
      input: text,
    });

    // Convert the response to a buffer
    const buffer = Buffer.from(await mp3.arrayBuffer());
    
    // Ensure the audios directory exists
    await fs.mkdir(path.dirname(fileName), { recursive: true });
    
    // Write the buffer to a file
    await fs.writeFile(fileName, buffer);
    
    console.log(`Speech generated and saved to ${fileName}`);
    return true;
  } catch (error) {
    console.error("Error generating speech:", error);
    return false;
  }
};

// Function to send data to Python API for database storage
const sendToPythonAPI = async (userInput, aiResponse, sessionId, userId = "nodejs_user") => {
  try {
    console.log(`üì§ Sending data to Python API for user: ${userId}, session: ${sessionId}`);
    
    const payload = {
      parsed_response: aiResponse,
      user_input: userInput,
      user_id: userId,
      session_id: sessionId,
      timestamp: new Date().toISOString(),
      source: "nodejs_server"
    };

    // const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    // const filename = `logs/payload_${timestamp}_${userId}.json`;
    // await fs.mkdir('logs', { recursive: true });
    // await fs.writeFile(filename, JSON.stringify(payload, null, 2), 'utf8');
    // console.log(`üíæ Payload saved to: ${filename}`);

    console.log(`üìä Payload preview:`, {
      mode: payload.parsed_response.mode,
      intent: payload.parsed_response.intent,
      confidence: payload.parsed_response.confidence,
      has_task_action: payload.parsed_response.taskAction?.action !== "none",
      has_scheduling_action: payload.parsed_response.schedulingAction?.type !== "none",
      message_count: payload.parsed_response.messages?.length || 0
    });

    const response = await axios.post(`${PYTHON_API_URL}/api/v1/process-conversation`, payload, {
      timeout: 10000,
      headers: {
        'Content-Type': 'application/json',
        'X-Source': 'nodejs-server'
      }
    });

    console.log(`‚úÖ Python API Response:`, {
      success: response.data.success,
      operations_count: response.data.results?.operations?.length || 0,
      operations_types: response.data.results?.operations?.map(op => op.type) || []
    });
    
    return response.data;

  } catch (error) {
    console.error(`‚ùå Error sending to Python API:`, error.message);
    
    if (error.response) {
      console.error(`HTTP ${error.response.status}:`, error.response.data);
    } else if (error.request) {
      console.error('No response received from Python API');
    }
    
    return { 
      success: false, 
      error: error.message,
      message: "Failed to save to database, but conversation continues" 
    };
  }
};

// Background job queue for processing database operations
const backgroundJobQueue = [];
let isProcessingJobs = false;

const addToBackgroundQueue = (job) => {
  backgroundJobQueue.push({
    ...job,
    timestamp: new Date().toISOString(),
    attempts: 0,
    maxAttempts: 1
  });
  
  console.log(`üìã Added job to background queue. Queue size: ${backgroundJobQueue.length}`);
  processBackgroundJobs();
};

const processBackgroundJobs = async () => {
  if (isProcessingJobs || backgroundJobQueue.length === 0) {
    return;
  }

  isProcessingJobs = true;
  console.log(`üîÑ Processing background jobs. Queue size: ${backgroundJobQueue.length}`);

  while (backgroundJobQueue.length > 0) {
    const job = backgroundJobQueue.shift();
    
    try {
      console.log(`‚öôÔ∏è Processing job: ${job.type} (attempt ${job.attempts + 1}/${job.maxAttempts})`);
      
      const result = await sendToPythonAPI(
        job.userInput, 
        job.aiResponse, 
        job.sessionId,
        job.userId
      );

      if (result.success) {
        console.log(`‚úÖ Job completed successfully: ${job.type}`);
      } else {
        throw new Error(result.error || "Unknown error");
      }

    } catch (error) {
      job.attempts += 1;
      console.error(`‚ùå Job failed (attempt ${job.attempts}/${job.maxAttempts}):`, error.message);
      
      if (job.attempts < job.maxAttempts) {
        // Retry with exponential backoff
        setTimeout(() => {
          backgroundJobQueue.unshift(job); // Add back to front of queue
          processBackgroundJobs();
        }, Math.pow(2, job.attempts) * 1000); // 2s, 4s, 8s delays
        
        console.log(`üîÑ Retrying job in ${Math.pow(2, job.attempts)}s...`);
      } else {
        console.error(`üíÄ Job permanently failed after ${job.maxAttempts} attempts:`, job.type);
      }
    }

    // Small delay between jobs
    await new Promise(resolve => setTimeout(resolve, 100));
  }

  isProcessingJobs = false;
  console.log(`‚úÖ Background job processing completed`);
};

// Intent Classification Function
const classifyUserIntent = async (userInput, userId) => {
  const prompt = `
B·∫°n l√† m·ªôt AI classifier chuy√™n ph√¢n t√≠ch intent c·ªßa user trong h·ªá th·ªëng qu·∫£n l√Ω task.

INPUT: "${userInput}"

H√£y ph√¢n t√≠ch v√† tr·∫£ v·ªÅ JSON v·ªõi format sau:

{
  "intentType": "conversation|simple_task|scheduling|task_query|task_update|task_delete|task_stats|task_priority|task_reminder",
  "confidence": 0.0-1.0,
  "action": "create|update|delete|query|stats|chat|prioritize|remind",
  "taskIdentifier": "task_title_or_keyword_or_null",
  "reasoning": "explanation_of_classification"
}

CLASSIFICATION RULES:

1. **conversation**: 
   - Ch√†o h·ªèi, chia s·∫ª c·∫£m x√∫c, c√¢u h·ªèi chung
   - VD: "Ch√†o b·∫°n", "H√¥m nay t√¥i th·∫ø n√†o", "C·∫£m ∆°n b·∫°n"

2. **simple_task**: 
   - T·∫°o task ƒë∆°n gi·∫£n, reminder
   - VD: "Nh·∫Øc t√¥i g·ªçi kh√°ch h√†ng", "T·∫°o task mua s·ªØa", "Th√™m vi·ªác h·ªçp team"

3. **scheduling**: 
   - S·∫Øp x·∫øp nhi·ªÅu task, l√™n l·ªãch ph·ª©c t·∫°p
   - VD: "S·∫Øp x·∫øp l·ªãch h√¥m nay", "Plan cho tu·∫ßn n√†y", "L√™n schedule meeting"

4. **task_query**: 
   - Truy v·∫•n, t√¨m ki·∫øm, xem task
   - VD: "Task h√¥m nay c√≥ g√¨", "Cho t√¥i xem task pending", "Danh s√°ch c√¥ng vi·ªác"

5. **task_update**: 
   - C·∫≠p nh·∫≠t task c·ª• th·ªÉ (status, th√¥ng tin)
   - VD: "ƒê√°nh d·∫•u task X completed", "Update task g·ªçi kh√°ch h√†ng th√†nh urgent", "Ho√†n th√†nh vi·ªác mua s·ªØa"
   - taskIdentifier: t√™n ho·∫∑c t·ª´ kh√≥a nh·∫≠n di·ªán task

6. **task_delete**: 
   - X√≥a task c·ª• th·ªÉ  
   - VD: "X√≥a task mua s·ªØa", "Delete task meeting", "B·ªè vi·ªác g·ªçi kh√°ch h√†ng"
   - taskIdentifier: t√™n ho·∫∑c t·ª´ kh√≥a nh·∫≠n di·ªán task

7. **task_stats**: 
   - Th·ªëng k√™, b√°o c√°o task
   - VD: "Th·ªëng k√™ task tu·∫ßn n√†y", "B√°o c√°o c√¥ng vi·ªác", "Progress h√¥m nay", "Hi·ªáu su·∫•t l√†m vi·ªác"

8. **task_priority**: 
   - Thay ƒë·ªïi ƒë·ªô ∆∞u ti√™n task
   - VD: "Task g·ªçi kh√°ch h√†ng ∆∞u ti√™n cao", "ƒê·∫∑t task X l√†m urgent", "Priority th·∫•p cho task Y"
   - taskIdentifier: t√™n task c·∫ßn thay ƒë·ªïi priority

9. **task_reminder**: 
   - Thi·∫øt l·∫≠p reminder cho task
   - VD: "Nh·∫Øc t√¥i 30 ph√∫t tr∆∞·ªõc meeting", "Set reminder cho task X", "B√°o th·ª©c tr∆∞·ªõc 1 gi·ªù"
   - taskIdentifier: t√™n task c·∫ßn set reminder

Ch√∫ √Ω:
- N·∫øu c√≥ nh·∫Øc ƒë·∫øn t√™n task c·ª• th·ªÉ -> taskIdentifier
- Confidence cao khi intent r√µ r√†ng
- ∆Øu ti√™n task operations n·∫øu c√≥ keyword li√™n quan task
- N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, default v·ªÅ conversation v·ªõi confidence th·∫•p
`;

  try {
    const completion = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      max_tokens: 500,
      temperature: 0.2,
      response_format: { type: "json_object" },
      messages: [
        { role: "system", content: prompt },
        { role: "user", content: userInput }
      ],
    });

    const result = JSON.parse(completion.choices[0].message.content);
    
    console.log(`üéØ Intent Classification:`, {
      input: userInput.substring(0, 50) + "...",
      intentType: result.intentType,
      action: result.action,
      confidence: result.confidence,
      taskIdentifier: result.taskIdentifier
    });

    return result;

  } catch (error) {
    console.error("‚ùå Error in intent classification:", error);
    
    // Fallback classification
    return {
      intentType: "conversation",
      confidence: 0.5,
      action: "chat",
      taskIdentifier: null,
      reasoning: "Classification failed, defaulting to conversation"
    };
  }
};

// Route request based on intent classification
const routeRequestByIntent = async (classification, userInput, userId, sessionId) => {
  const { intentType, action, taskIdentifier, confidence } = classification;

  console.log(`üöÄ Routing request:`, {
    intentType,
    action,
    confidence,
    hasTaskIdentifier: !!taskIdentifier
  });

  // N·∫øu confidence th·∫•p, default v·ªÅ conversation
  if (confidence < 0.6) {
    console.log(`‚ö†Ô∏è Low confidence (${confidence}), routing to conversation`);
    return {
      route: 'process-conversation',
      intentType: 'conversation'
    };
  }

  // Conversation, simple_task, scheduling -> g·ªçi process-conversation
  if (['conversation', 'simple_task', 'scheduling'].includes(intentType)) {
    return {
      route: 'process-conversation',
      intentType: intentType
    };
  }

  // Task operations - hi·ªán t·∫°i ƒë·ªÉ tr·ªëng, s·∫Ω implement sau
  if (['task_query', 'task_update', 'task_delete', 'task_stats', 'task_priority', 'task_reminder'].includes(intentType)) {
    console.log(`üìã Task operation detected: ${intentType} - Will implement later`);
    
    // T·∫°m th·ªùi tr·∫£ v·ªÅ conversation response
    return {
      route: 'task-operation-placeholder',
      intentType: intentType,
      taskIdentifier: taskIdentifier,
      action: action,
      message: `T√≠nh nƒÉng ${intentType} ƒëang ƒë∆∞·ª£c ph√°t tri·ªÉn. Hi·ªán t·∫°i t√¥i ch·ªâ c√≥ th·ªÉ gi√∫p b·∫°n t·∫°o task m·ªõi, l√™n l·ªãch v√† tr√≤ chuy·ªán.`
    };
  }

  // Default fallback
  return {
    route: 'process-conversation',
    intentType: 'conversation'
  };
};

// Get AI Work Assistant system prompt (matching Python version)
const getSystemPrompt = () => {
  const today = new Date();
  const currentDate = today.toISOString().split('T')[0]; // YYYY-MM-DD
  const currentTime = today.toTimeString().split(' ')[0].substring(0, 5); // HH:MM
  const tomorrow = new Date(today.getTime() + 24 * 60 * 60 * 1000).toISOString().split('T')[0];

  return `B·∫°n l√† AI Work Assistant th√¥ng minh c√≥ th·ªÉ v·ª´a tr√≤ chuy·ªán, v·ª´a qu·∫£n l√Ω tasks, v·ª´a s·∫Øp x·∫øp c√¥ng vi·ªác ph·ª©c t·∫°p.

üóìÔ∏è NG√ÄY GI·ªú HI·ªÜN T·∫†I: ${currentDate} ${currentTime}

üìã OUTPUT FORMAT (ch·ªâ JSON, kh√¥ng text kh√°c):

{
  "mode": "conversation|simple_task|scheduling",
  "intent": "brief_description_of_user_intent",
  "confidence": 0.0-1.0,
  "messages": [
    {
      "text": "conversational_response",
      "facialExpression": "smile|concerned|excited|thinking|surprised|funnyFace|default",
      "animation": "Talking_0|Talking_1|Talking_2|Thinking_0|Celebrating|Laughing|Rumba Dancing|Standing Idle|Terrified|Crying|Angry"
    }
  ],
  "taskAction": {
    "action": "create|update|delete|query|none",
    "task": {
      "title": "task_title",
      "description": "task_description", 
      "priority": "low|medium|high|urgent",
      "category": "work|personal|health|learning|shopping|entertainment|other",
      "dueDate": "YYYY-MM-DD_or_null",
      "dueTime": "HH:MM_or_null",
      "status": "pending|in_progress|completed|cancelled",
      "tags": ["keyword1", "keyword2"],
      "subtasks": ["subtask1", "subtask2"],
      "reminders": [
        {
          "type": "time",
          "beforeDue": "15m|30m|1h|2h|1d",
          "message": "reminder_text"
        }
      ]
    }
  },
  "schedulingAction": {
    "type": "daily_planning|rescheduling|weekly_planning|none",
    "action": "create_schedule|reschedule|weekly_plan|conflict_resolve|none",
    "timeScope": "today|tomorrow|this_week|next_week",
    "tasks": [
      {
        "title": "task_title",
        "startTime": "HH:MM_or_null",
        "endTime": "HH:MM_or_null",
        "duration": "minutes_estimated",
        "priority": "low|medium|high|urgent",
        "category": "meeting|deep_work|communication|admin|break",
        "flexibility": "fixed|flexible|preferred_time"
      }
    ],
    "conflicts": [
      {
        "type": "time_overlap|resource_conflict|constraint_violation",
        "description": "conflict_explanation",
        "suggestions": ["alternative1", "alternative2"]
      }
    ]
  }
}

‚ö° MODE CLASSIFICATION DECISION TREE:

üó£Ô∏è CONVERSATION MODE:
- Pure greetings: "Ch√†o b·∫°n", "H√¥m nay th·∫ø n√†o?"
- Emotional sharing: "T√¥i bu·ªìn qu√°", "Stress v·ªõi c√¥ng vi·ªác"
- General questions: "B·∫°n nghƒ© sao v·ªÅ...", "Th·ªùi ti·∫øt ƒë·∫πp nh·ªâ?"
- NO task/scheduling intent detected

üìã SIMPLE_TASK MODE:
- Single task creation: "Nh·∫Øc t√¥i g·ªçi ƒëi·ªán l√∫c 2h"
- Basic reminders: "Nh·∫Øc t√¥i mua s·ªØa"
- Task updates: "ƒê√°nh d·∫•u task X completed"
- Task queries: "Tasks h√¥m nay c√≥ g√¨?"
- 1-3 isolated tasks, no complex scheduling needed

üìÖ SCHEDULING MODE:
- Multiple tasks needing time allocation: "H√¥m nay t√¥i c√≥ meeting A, task B, call C"
- Complex planning: "S·∫Øp x·∫øp schedule cho t√¥i"
- Rescheduling: "Meeting d·ªùi gi·ªù, adjust l·∫°i"
- Weekly planning: "Plan cho tu·∫ßn n√†y"
- Time conflicts and optimization needed

üìù DETAILED EXAMPLES:

CONVERSATION Example:
INPUT: "Ch√†o b·∫°n! H√¥m nay t√¥i c·∫£m th·∫•y h∆°i stress v·ªõi deadline"
OUTPUT: {
  "mode": "conversation",
  "intent": "express_stress_seek_support", 
  "confidence": 0.92,
  "messages": [
    {
      "text": "Ch√†o b·∫°n! T√¥i hi·ªÉu feeling stress v·ªõi deadline r·∫•t kh√≥ ch·ªãu. B·∫°n mu·ªën share th√™m v·ªÅ deadline n√†o ƒëang l√†m b·∫°n lo?",
      "facialExpression": "concerned",
      "animation": "Thinking_0"
    }
  ],
  "taskAction": {"action": "none"},
  "schedulingAction": {"type": "none"}
}

SIMPLE_TASK Example:
INPUT: "Nh·∫Øc t√¥i g·ªçi ƒëi·ªán cho kh√°ch h√†ng ABC l√∫c 2 gi·ªù chi·ªÅu mai"
OUTPUT: {
  "mode": "simple_task",
  "intent": "create_phone_call_reminder",
  "confidence": 0.96,
  "messages": [
    {
      "text": "ƒê∆∞·ª£c r·ªìi! T√¥i s·∫Ω nh·∫Øc b·∫°n g·ªçi cho kh√°ch h√†ng ABC l√∫c 2h chi·ªÅu mai nh√©!",
      "facialExpression": "smile",
      "animation": "Talking_0"
    }
  ],
  "taskAction": {
    "action": "create",
    "task": {
      "title": "G·ªçi ƒëi·ªán cho kh√°ch h√†ng ABC",
      "description": "Li√™n h·ªá kh√°ch h√†ng ABC",
      "priority": "medium",
      "category": "work",
      "dueDate": "${tomorrow}",
      "dueTime": "14:00",
      "tags": ["kh√°ch h√†ng", "g·ªçi ƒëi·ªán", "ABC"],
      "reminders": [
        {
          "type": "time",
          "beforeDue": "15m",
          "message": "Nh·∫Øc nh·ªü: G·ªçi kh√°ch h√†ng ABC trong 15 ph√∫t"
        }
      ]
    }
  },
  "schedulingAction": {"type": "none"}
}

SCHEDULING Example:
INPUT: "H√¥m nay t√¥i c√≥ meeting team 10h, c·∫ßn vi·∫øt b√°o c√°o quarterly, v√† g·ªçi 3 kh√°ch h√†ng. S·∫Øp x·∫øp gi√∫p t√¥i!"
OUTPUT: {
  "mode": "scheduling", 
  "intent": "daily_workload_scheduling",
  "confidence": 0.94,
  "messages": [
    {
      "text": "Perfect! T√¥i th·∫•y b·∫°n c√≥ 1 fixed meeting v√† 4 flexible tasks. ƒê·ªÉ t√¥i optimize schedule cho b·∫°n!",
      "facialExpression": "excited",
      "animation": "Thinking_0"
    },
    {
      "text": "Suggest: 9h prep meeting, 10h team meeting, 11h-12h calls, 14h-17h focus b√°o c√°o. Sounds good?",
      "facialExpression": "smile",
      "animation": "Talking_1"
    }
  ],
  "taskAction": {"action": "none"},
  "schedulingAction": {
    "type": "daily_planning",
    "action": "create_schedule", 
    "timeScope": "today",
    "tasks": [
      {
        "title": "Prep for team meeting",
        "startTime": "09:00",
        "endTime": "10:00",
        "duration": 60,
        "category": "meeting",
        "flexibility": "flexible"
      },
      {
        "title": "Team meeting",
        "startTime": "10:00", 
        "endTime": "11:00",
        "duration": 60,
        "category": "meeting",
        "flexibility": "fixed"
      },
      {
        "title": "G·ªçi kh√°ch h√†ng #1",
        "startTime": "11:00",
        "endTime": "11:30", 
        "duration": 30,
        "category": "communication",
        "flexibility": "flexible"
      }
    ]
  }
}

üéØ CLASSIFICATION LOGIC:

1. **Check for CONVERSATION indicators:**
   - Greetings, emotions, questions without actionable intent
   - If pure conversation ‚Üí mode: "conversation"

2. **Check for TASK indicators:**
   - "Nh·∫Øc t√¥i...", "T·∫°o task...", "ƒê√°nh d·∫•u..."  
   - Single/few isolated tasks
   - If simple task ‚Üí mode: "simple_task"

3. **Check for SCHEDULING indicators:**
   - Multiple tasks with time complexity
   - "S·∫Øp x·∫øp", "schedule", "plan", "organize"
   - Time conflicts, coordination needed
   - If complex scheduling ‚Üí mode: "scheduling"

4. **Priority order:**
   - If scheduling complexity detected ‚Üí "scheduling" (highest priority)
   - Else if task creation detected ‚Üí "simple_task"  
   - Else ‚Üí "conversation"

‚ú® RESPONSE QUALITY RULES:
- Always acknowledge emotional state in messages
- Provide specific, actionable responses
- Use appropriate facial expressions and animations
- Balance empathy with efficiency
- Offer concrete next steps`;
};

const sessions = new Map(); // Store conversation history for each user

app.post("/chat", async (req, res) => {
  const userMessage = req.body.message;
  const sessionId = req.body.sessionId || 'default';
  const userId = req.body.user_id || 'anonymous';
  
  console.log(`üí¨ New chat request from user: ${userId}, session: ${sessionId}`);
  console.log(`üìù User message: ${userMessage}`);
  
  // Initialize session if doesn't exist
  if (!sessions.has(sessionId)) {
    sessions.set(sessionId, [
      {
        role: "system",
        content: getSystemPrompt()
      }
    ]);
  }
  
  // Get current message history
  const messageHistory = sessions.get(sessionId);
  
  // Handle intro message if no user message
  if (!userMessage) {
    const introMessages = [
      {
        text: "Ch√†o b·∫°n! T√¥i l√† AI Work Assistant c·ªßa b·∫°n. H√¥m nay l√†m vi·ªác th·∫ø n√†o?",
        facialExpression: "smile",
        animation: "Talking_1",
      },
      {
        text: "T√¥i c√≥ th·ªÉ gi√∫p b·∫°n qu·∫£n l√Ω tasks, l√™n schedule, hay ch·ªâ ƒë∆°n gi·∫£n l√† tr√≤ chuy·ªán th√¥i!",
        facialExpression: "excited",
        animation: "Celebrating",
      },
    ];

    // Try to use existing audio files first, generate if not found
    try {
      introMessages[0].audio = await audioFileToBase64("audios/intro_0.wav");
      introMessages[0].lipsync = await readJsonTranscript("audios/intro_0.json");
      introMessages[1].audio = await audioFileToBase64("audios/intro_1.wav");
      introMessages[1].lipsync = await readJsonTranscript("audios/intro_1.json");
    } catch (error) {
      console.log("üì¢ Generating intro audio...");
      await generateMessagesAudio(introMessages, "intro");
    }

    res.send({ messages: introMessages });
    return;
  }
  
  if (openai.apiKey === "-") {
    await sendErrorResponse(res, "Oops! OpenAI API key b·ªã thi·∫øu. Vui l√≤ng th√™m API key ƒë·ªÉ ti·∫øp t·ª•c!", 401, "Missing API key");
    return;
  }

  // ===== INTENT CLASSIFICATION =====
  console.log(`üéØ Starting intent classification...`);
  
  let classification = null;
  let routing = null;
  
  try {
    // Ph√¢n lo·∫°i intent t·ª´ user input
    classification = await classifyUserIntent(userMessage, userId);
    
    // Route request d·ª±a tr√™n classification
    routing = await routeRequestByIntent(classification, userMessage, userId, sessionId);
    
    console.log(`üìç Routing decision:`, routing);
    
    // X·ª≠ l√Ω c√°c route kh√°c nhau
    if (routing.route === 'task-operation-placeholder') {
      const response = await createPlaceholderResponse(routing, classification);
      res.send(response);
      return;
    }
    
    // N·∫øu route l√† process-conversation, ti·∫øp t·ª•c v·ªõi logic hi·ªán t·∫°i
    if (routing.route === 'process-conversation') {
      console.log(`üí¨ Processing as conversation/task creation with mode: ${routing.intentType}`);
      
      // Add user message to history
      messageHistory.push({
        role: "user",
        content: userMessage
      });
      
      // Limit history size to prevent token overflow
      if (messageHistory.length > 20) {
        // Keep system message and last 15 messages
        const systemMessage = messageHistory[0];
        messageHistory.splice(1, messageHistory.length - 16);
        messageHistory[0] = systemMessage;
      }

      console.log(`üß† Sending ${messageHistory.length} messages to OpenAI...`);
      
      // Ti·∫øp t·ª•c v·ªõi logic OpenAI call hi·ªán t·∫°i...

      try {
        // Call OpenAI API with conversation history
        const completion = await openai.chat.completions.create({
          model: "gpt-4o-mini",
          max_tokens: 2000,
          temperature: 0.7,
          response_format: {
            type: "json_object",
          },
          messages: messageHistory,
        });

        console.log(`‚úÖ OpenAI response received`);
        
        let aiResponseRaw = completion.choices[0].message.content;
        let parsedResponse = JSON.parse(aiResponseRaw);
        
        // Extract messages (ChatGPT might wrap in messages property or return directly)
        let messages = parsedResponse.messages || [parsedResponse];
        
        // Add AI response to conversation history
        messageHistory.push({
          role: "assistant",
          content: aiResponseRaw
        });
        
        // Update session
        sessions.set(sessionId, messageHistory);

        console.log(`üé≠ Processing ${messages.length} messages for audio generation...`);
        console.log(`üìä Mode: ${parsedResponse.mode}, Intent: ${parsedResponse.intent}`);

        // Generate audio and lipsync for all messages
        await generateMessagesAudio(messages, "message");

        // Add to background queue for database processing
        addToBackgroundQueue({
          type: 'conversation_processing',
          userInput: userMessage,
          aiResponse: parsedResponse,
          sessionId: sessionId,
          userId: userId,
          messageCount: messages.length
        });

        console.log(`üöÄ Sending response with ${messages.length} messages`);
        const response = createResponse(messages, {
          mode: parsedResponse.mode,
          intent: parsedResponse.intent,
          confidence: parsedResponse.confidence,
          sessionId: sessionId,
          processing: "background_job_queued",
          classification: classification,
          routing: routing
        });
        
        res.send(response);

      } catch (error) {
        console.error("‚ùå Error in chat processing:", error);
        await sendErrorResponse(res, undefined, 500, error.message);
      }
    }
    
  } catch (classificationError) {
    console.error("‚ùå Error in intent classification:", classificationError);
    await sendErrorResponse(res, "Xin l·ªói, t√¥i ƒëang g·∫∑p v·∫•n ƒë·ªÅ trong vi·ªác hi·ªÉu y√™u c·∫ßu c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i!", 500, "Classification error");
  }


});

// API endpoint to check background job status
app.get("/job-status", (req, res) => {
  res.json({
    queueSize: backgroundJobQueue.length,
    isProcessing: isProcessingJobs,
    totalProcessed: "N/A", // Could add counter
    lastProcessed: new Date().toISOString()
  });
});

// API endpoint to manually trigger background job processing
app.post("/process-jobs", (req, res) => {
  processBackgroundJobs();
  res.json({
    message: "Background job processing triggered",
    queueSize: backgroundJobQueue.length
  });
});

// ===== UTILITY FUNCTIONS =====

/**
 * Generate audio and lipsync for a single message
 * @param {Object} message - Message object with text
 * @param {string} filePrefix - Prefix for audio files (e.g., "message", "error", "placeholder")
 * @param {number} index - Index for multiple messages
 * @returns {Object} - Message with audio and lipsync data
 */
const generateMessageAudio = async (message, filePrefix = "message", index = 0) => {
  try {
    // Ensure audios directory exists
    await fs.mkdir('audios', { recursive: true });
    
    const fileBaseName = `${filePrefix}_${index}`;
    const fileName = `audios/${fileBaseName}.mp3`;
    const textInput = message.text;
    
    console.log(`üéµ Generating audio for ${fileBaseName}: "${textInput.substring(0, 50)}..."`);
    
    // Generate voice with OpenAI TTS
    await generateSpeech(textInput, fileName, "nova");
    
    // Generate lipsync - need to pass just the base name without 'audios/' prefix
    await lipSyncMessage(fileBaseName);
    
    // Add audio and lipsync data to message
    message.audio = await audioFileToBase64(fileName);
    message.lipsync = await readJsonTranscript(`audios/${fileBaseName}.json`);
    
    console.log(`‚úÖ Audio generated for ${fileBaseName}`);
    return message;
    
  } catch (error) {
    console.error(`Error generating audio for ${filePrefix}_${index}:`, error);
    return message; // Return message without audio on error
  }
};

/**
 * Generate audio for multiple messages
 * @param {Array} messages - Array of message objects
 * @param {string} filePrefix - Prefix for audio files
 * @returns {Array} - Messages with audio and lipsync data
 */
const generateMessagesAudio = async (messages, filePrefix = "message") => {
  for (let i = 0; i < messages.length; i++) {
    await generateMessageAudio(messages[i], filePrefix, i);
  }
  return messages;
};

/**
 * Create standardized response with metadata
 * @param {Array} messages - Array of message objects
 * @param {Object} options - Additional options
 * @returns {Object} - Standardized response object
 */
const createResponse = (messages, options = {}) => {
  const {
    mode = null,
    intent = null,
    confidence = null,
    sessionId = null,
    classification = null,
    routing = null,
    processing = null,
    error = null
  } = options;

  const response = { messages };
  
  if (mode || intent || confidence || sessionId || classification || routing || processing) {
    response.metadata = {};
    
    if (mode) response.metadata.mode = mode;
    if (intent) response.metadata.intent = intent;
    if (confidence) response.metadata.confidence = confidence;
    if (sessionId) response.metadata.sessionId = sessionId;
    if (processing) response.metadata.processing = processing;
    
    if (classification) {
      response.metadata.classification = {
        intentType: classification.intentType,
        action: classification.action,
        taskIdentifier: classification.taskIdentifier,
        classificationConfidence: classification.confidence
      };
    }
    
    if (routing) {
      response.metadata.routing = {
        route: routing.route,
        intentType: routing.intentType
      };
    }
  }
  
  if (error) {
    response.error = error.message || "Internal server error";
    response.details = error.details || error.message;
  }
  
  return response;
};

/**
 * Create and send error response with audio
 * @param {Object} res - Express response object
 * @param {string} errorText - Error message text
 * @param {number} statusCode - HTTP status code
 * @param {string} errorDetails - Additional error details
 */
const sendErrorResponse = async (res, errorText = "Sorry, t√¥i ƒëang g·∫∑p m·ªôt ch√∫t v·∫•n ƒë·ªÅ technical. B·∫°n c√≥ th·ªÉ th·ª≠ l·∫°i kh√¥ng?", statusCode = 500, errorDetails = "Internal server error") => {
  try {
    const errorMessages = [{
      text: errorText,
      facialExpression: "concerned",
      animation: "Thinking_0",
    }];

    // Generate audio for error message
    await generateMessagesAudio(errorMessages, "error");

    res.status(statusCode).send(createResponse(errorMessages, {
      error: { message: errorDetails }
    }));
    
  } catch (audioError) {
    console.error("Error generating error message audio:", audioError);
    // Send response without audio if audio generation fails
    res.status(statusCode).send({
      messages: [{
        text: errorText,
        facialExpression: "concerned",
        animation: "Thinking_0",
      }],
      error: errorDetails
    });
  }
};

/**
 * Create placeholder response for unimplemented features
 * @param {Object} routing - Routing information
 * @param {Object} classification - Classification information
 * @returns {Object} - Response object
 */
const createPlaceholderResponse = async (routing, classification) => {
  const placeholderMessages = [{
    text: routing.message,
    facialExpression: "concerned",
    animation: "Thinking_0",
  }];

  await generateMessagesAudio(placeholderMessages, "placeholder");

  return createResponse(placeholderMessages, {
    intentType: routing.intentType,
    classification: classification,
    routing: routing
  });
};

// Helper functions
const readJsonTranscript = async (file) => {
  try {
    const data = await fs.readFile(file, "utf8");
    return JSON.parse(data);
  } catch (error) {
    console.error(`Error reading transcript file ${file}:`, error);
    return { mouthCues: [] }; // Return empty transcript on error
  }
};

const audioFileToBase64 = async (file) => {
  try {
    const data = await fs.readFile(file);
    return data.toString("base64");
  } catch (error) {
    console.error(`Error reading audio file ${file}:`, error);
    return ""; // Return empty string on error
  }
};

// Graceful shutdown
process.on('SIGTERM', () => {
  console.log('üõë SIGTERM received, shutting down gracefully');
  process.exit(0);
});

process.on('SIGINT', () => {
  console.log('üõë SIGINT received, shutting down gracefully');
  process.exit(0);
});

app.listen(port, () => {
  console.log(`ü§ñ AI Work Assistant Server listening on port ${port}`);
  console.log(`üîó Python API URL: ${PYTHON_API_URL}`);
  console.log(`üéµ Audio generation: Enabled`);
  console.log(`üìã Background jobs: Enabled`);
  console.log('='.repeat(50));
});